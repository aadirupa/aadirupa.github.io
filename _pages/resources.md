---
layout: archive
title: ""
permalink: /resources/
author_profile: true
---

<html>
<head>
<style>
a:link {
  color: RoyalBlue;
  background-color: transparent;
  text-decoration: none;
}

a:visited {
  color: Purple;
  background-color: transparent;
  text-decoration: none;
}

a:hover {
  color: RoyalBlue;
  background-color: transparent;
  text-decoration: underline;
}

a:active {
  color: DarkRed;
  background-color: transparent;
  text-decoration: underline;
}
</style>  
</head>  
 
<body>  

<p style="text-align:center;"> ----- The page is under construction, please check again for updates. Suggestions are most welcome! ----- </p> 

<h2 style="color:DarkBlue;" vspace="-2px;">Dueling Bandits (Learning from Preferences)</h2>

<ul type="1">
<li>Some exciting blogs: 
  (1). <a href="https://openai.com/research/learning-from-human-preferences" LINK="red">Reward design problem in RL</a> (OpenAI),
  (2). <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/" LINK="red">Stop your Robots from making a mess!</a> (BAIR),  
  (3). <a href="https://openai.com/research/learning-to-summarize-with-human-feedback" LINK="red">Learning to summarize with human preferences</a> (OpenAI),
  (4). <a href="https://openai.com/research/instruction-following" LINK="red">Aligning language models with preference feedback!</a> (OpenAI). 
  There are a lot more available online to get you excited!
</li>  
<li> <p align="justify" vspace = "-0px" width="200px"> Some breakthrough results (non-inclusive):
  <a href="https://www.cs.cornell.edu/people/tj/publications/yue_etal_09a.pdf" LINK="red">(1). First Work!</a>,
  <a href="https://proceedings.mlr.press/v32/zoghi14.html" LINK="red">(2). Dueling Bandits with simple UCB</a>,
  <a href="http://proceedings.mlr.press/v32/ailon14.pdf" LINK="red">(3). Reducing Dueling Bandits to Standard MAB</a>,
  <a href="https://arxiv.org/abs/2210.14322" LINK="red">(4). Dynamic Dueling</a>,
  <a href="https://arxiv.org/abs/2010.14563" LINK="red">(5). Adversarial Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v162/saha22a.html" LINK="red">(6). Best of Both Dueling Bandits</a>,
  <a href="http://proceedings.mlr.press/v139/saha21b/saha21b.pdf" LINK="red">(7). Optimization with Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v167/saha22a/saha22a.pdf" LINK="red">(8). Optimal Rates for Contextual Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v206/saha23a/saha23a.pdf" LINK="red">(8). RL with Dueling Feedback</a>.</p>
</li>  
<li> Excellent Survey: <a href="https://arxiv.org/abs/1807.11398" LINK="red">Preference-based Online Learning with Dueling Bandits</a></li>
<li> Book: <a href="https://www.google.com/books/edition/Preference_Learning/nc3XcH9XSgYC?hl=en&gbpv=0" LINK="red">Preference Learning</a></li>
<li> Video talk: <a href="https://www.youtube.com/watch?v=tW9OuZ_-tYs" LINK="red">Preference Learning</a> (by Eyke Hullermeier) </li>
  
</ul> 
  
<h2 style="color:DarkBlue;" vspace="-2px;">Online Learning</h2>

<ul type="1">
<li> A great course (with many references and lecture notes): <a href="https://ece.iisc.ac.in/~aditya/E1245_F15.html" LINK="red">Online Prediction & Learning</a> (by Aditya Gopalan) </li>
<li> Book: <a href="https://ece.iisc.ac.in/~aditya/E1245_F15.html" LINK="red">Prediction Learning & Games</a> (Nicolo Cesa-Bianchi and Gabor Lugosi)</li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Bandits and Reinforcement Learning</h2>

<ul type="1">
<li> Few amazing books:  
(1) <a href="https://banditalgs.com/" LINK="red">Bandit Algorithms</a> (by Tor Lattimore, Csaba Szepesvari),
(2) <a href="" LINK="red">Reinforcement Learning</a> (by Richard Sutton & Andrew Barto) 
</li>
<li> Course with comprehensive notes: <a href="https://people.cs.umass.edu/~akshay/courses/coms6998-11/index.html" LINK="red">Bandits and Reinforcement Learning</a> (by Akshay Krishnamurthy)</li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Optimization</h2>

<ul type="1">
<li> Classical books: </li>  
<li> Notes on Convex Optimization: . Also a blog </li>
<li> Notes on Online Optimization: </li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Federated Learning</h2>

<ul type="1">
<li> --- </li>
</ul>

<h2 style="color:DarkBlue;" vspace="-2px;">Differential Privacy</h2>

<ul type="1">
<li> --- </li>
</ul>

</body>
</html>
