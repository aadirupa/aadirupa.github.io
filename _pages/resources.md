---
layout: archive
title: ""
permalink: /resources/
author_profile: true
---

<html>
<head>
<style>
a:link {
  color: RoyalBlue;
  background-color: transparent;
  text-decoration: none;
}

a:visited {
  color: Purple;
  background-color: transparent;
  text-decoration: none;
}

a:hover {
  color: RoyalBlue;
  background-color: transparent;
  text-decoration: underline;
}

a:active {
  color: DarkRed;
  background-color: transparent;
  text-decoration: underline;
}
</style>  
</head>  
 
<body>  

<p style="text-align:center;"> <font color="#1E90FF">----- The page is under construction, please check again for updates. Suggestions are most welcome! ----- </font></p> 

<h2 style="color:DarkBlue;" vspace="-2px;">Battle of Bandits (Learning from Preferences)</h2>

<ul type="1">
<li><p align="justify" vspace = "-0px" width="200px">Some exciting blogs: 
  (1). <a href="https://openai.com/research/learning-from-human-preferences" LINK="red">Reward design problem in RL</a> (OpenAI),
  (2). <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/" LINK="red">Stop your Robots from making a mess!</a> (BAIR),  
  (3). <a href="https://openai.com/research/learning-to-summarize-with-human-feedback" LINK="red">Learning to summarize with human preferences</a> (OpenAI),
  (4). <a href="https://openai.com/research/instruction-following" LINK="red">Aligning language models with preference feedback!</a> (OpenAI). 
  There are a lot more available online to get you excited!
</p></li>  
<li> <p align="justify" vspace = "-0px" width="200px"> Some breakthrough results (non-inclusive):
  <a href="https://www.cs.cornell.edu/people/tj/publications/yue_etal_09a.pdf" LINK="red">(1). First Work!</a>,
  <a href="https://proceedings.mlr.press/v32/zoghi14.html" LINK="red">(2). Dueling Bandits with simple UCB</a>,
  <a href="http://proceedings.mlr.press/v32/ailon14.pdf" LINK="red">(3). Reducing Dueling Bandits to Standard MAB</a>,
  <a href="https://arxiv.org/abs/2210.14322" LINK="red">(4). Dynamic Dueling</a>,
  <a href="https://arxiv.org/abs/2010.14563" LINK="red">(5). Adversarial Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v162/saha22a.html" LINK="red">(6). Best of Both Dueling Bandits</a>,
  <a href="http://proceedings.mlr.press/v139/saha21b/saha21b.pdf" LINK="red">(7). Optimization with Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v167/saha22a/saha22a.pdf" LINK="red">(8). Optimal Rates for Contextual Dueling Bandits</a>,
  <a href="https://proceedings.mlr.press/v206/saha23a/saha23a.pdf" LINK="red">(8). RL with Dueling Feedback</a>.</p>
</li>  
<li> Excellent Survey: <a href="https://arxiv.org/abs/1807.11398" LINK="red">Preference-based Online Learning with Dueling Bandits</a></li>
<li> Book: <a href="https://www.google.com/books/edition/Preference_Learning/nc3XcH9XSgYC?hl=en&gbpv=0" LINK="red">Preference Learning</a></li>
<li> Video talk: <a href="https://www.youtube.com/watch?v=tW9OuZ_-tYs" LINK="red">Preference Learning</a> (by Eyke Hullermeier) </li>
  
</ul> 
  
<h2 style="color:DarkBlue;" vspace="-2px;">Online Learning</h2>

<ul type="1">
<li> A great course (with many references and lecture notes): <a href="https://ece.iisc.ac.in/~aditya/E1245_F15.html" LINK="red">Online Prediction & Learning</a> (by Aditya Gopalan) </li>
<li> Book: <a href="https://ece.iisc.ac.in/~aditya/E1245_F15.html" LINK="red">Prediction Learning & Games</a> (by Nicolo Cesa-Bianchi and Gabor Lugosi)</li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Bandits and Reinforcement Learning</h2>

<ul type="1">
<li> Few amazing books:  
(1) <a href="https://banditalgs.com/" LINK="red">Bandit Algorithms</a> (by Tor Lattimore, Csaba Szepesvari), 
(2) <a href="https://arxiv.org/abs/1904.07272" LINK="red">Introduction to Multiarmed Bandits</a>,   
(3) <a href="http://incompleteideas.net/book/the-book-2nd.html" LINK="red">Reinforcement Learning: An Introduction</a> (by Richard Sutton & Andrew Barto),
(4) <a href="https://academic.oup.com/book/26549" LINK="red">Concentration Inequalities</a> (by Stephane Boucheron, Gabor Lugosi, Pascal Massart).</li>
<li> Course with comprehensive notes: <a href="https://people.cs.umass.edu/~akshay/courses/coms6998-11/index.html" LINK="red">Bandits and Reinforcement Learning</a> (by Akshay Krishnamurthy), <a href="https://www.cs.umd.edu/~slivkins/CMSC858G-fall16/" LINK="red">Bandits, Experts, and Games</a> (by Alex Slivkins)</li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Optimization</h2>

<ul type="1">
<li> Books: 
  (1). <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" LINK="red">Convex Optimization</a> (by Lieven Vandenberghe and Stephen P. Boyd),
  (2). <a href="https://www.nowpublishers.com/article/Details/MAL-058" LINK="red">Non-convex Optimization for Machine Learning </a>(by Lieven Vandenberghe and Stephen P. Boyd).
</li>  
<li> Notes on Convex Optimization: <a href="http://sbubeck.com/Bubeck15.pdf" LINK="red"></a> (by Sebastin Bubeck). Also a blog <a href="https://web.archive.org/web/20210123234450/https://blogs.princeton.edu/imabandit/" LINK="red">I'm a Bandit</a></li>
<li> Notes on Online Optimization: <a href="https://sites.google.com/view/intro-oco/" LINK="red">Introduction to
Online Convex Optimization</a> (by Elad Hazan).</li>  
</ul>  

<h2 style="color:DarkBlue;" vspace="-2px;">Federated Learning</h2>

<ul type="1">
<li> --- </li>
</ul>

<h2 style="color:DarkBlue;" vspace="-2px;">Differential Privacy</h2>

<ul type="1">
<li> --- </li>
</ul>

</body>
</html>
